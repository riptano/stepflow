name: ExampleFlow
description: 'Converted from Langflow using UDF execution. Original: Perform basic
  prompting with an OpenAI model.'
input_schema:
  type: object
  properties:
    message:
      type: string
      description: User input message
      default: Hello
  required:
  - message
steps:
- id: create_ChatInput-X99lk_udf
  component: builtin://put_blob
  input:
    data:
      input_schema:
        type: object
        properties:
          background_color:
            type: string
            description: The background color of the icon.
          chat_icon:
            type: string
            description: The icon of the message.
          code:
            type: string
            description: ''
          files:
            type: string
            description: Files to be sent with the message.
          input_value:
            type: string
            description: Message to be passed as input.
          sender:
            type: string
            description: Type of sender.
          sender_name:
            type: string
            description: Name of the sender.
          session_id:
            type: string
            description: The session ID of the chat. If empty, the current session
              ID parameter will be used.
          should_store_message:
            type: boolean
            description: Store the message in the history.
          text_color:
            type: string
            description: The text color of the name
        required:
        - code
      code: "# Auto-generated UDF code for ChatInput\n# This calls the langflow_component_server_udf_only.py\
        \ udf_executor\n\n# Extract the component's template configuration\ntemplate_config\
        \ = {'_type': 'Component', 'background_color': {'_input_type': 'MessageTextInput',\
        \ 'advanced': True, 'display_name': 'Background Color', 'dynamic': False,\
        \ 'info': 'The background color of the icon.', 'input_types': ['Message'],\
        \ 'list': False, 'load_from_db': False, 'name': 'background_color', 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'trace_as_input':\
        \ True, 'trace_as_metadata': True, 'type': 'str', 'value': ''}, 'chat_icon':\
        \ {'_input_type': 'MessageTextInput', 'advanced': True, 'display_name': 'Icon',\
        \ 'dynamic': False, 'info': 'The icon of the message.', 'input_types': ['Message'],\
        \ 'list': False, 'load_from_db': False, 'name': 'chat_icon', 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'trace_as_input':\
        \ True, 'trace_as_metadata': True, 'type': 'str', 'value': ''}, 'code': {'advanced':\
        \ True, 'dynamic': True, 'fileTypes': [], 'file_path': '', 'info': '', 'list':\
        \ False, 'load_from_db': False, 'multiline': True, 'name': 'code', 'password':\
        \ False, 'placeholder': '', 'required': True, 'show': True, 'title_case':\
        \ False, 'type': 'code', 'value': 'from langflow.base.data.utils import IMG_FILE_TYPES,\
        \ TEXT_FILE_TYPES\\nfrom langflow.base.io.chat import ChatComponent\\nfrom\
        \ langflow.inputs.inputs import BoolInput\\nfrom langflow.io import (\\n \
        \   DropdownInput,\\n    FileInput,\\n    MessageTextInput,\\n    MultilineInput,\\\
        n    Output,\\n)\\nfrom langflow.schema.message import Message\\nfrom langflow.utils.constants\
        \ import (\\n    MESSAGE_SENDER_AI,\\n    MESSAGE_SENDER_NAME_USER,\\n   \
        \ MESSAGE_SENDER_USER,\\n)\\n\\n\\nclass ChatInput(ChatComponent):\\n    display_name\
        \ = \"Chat Input\"\\n    description = \"Get chat inputs from the Playground.\"\
        \\n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\
        \\n    icon = \"MessagesSquare\"\\n    name = \"ChatInput\"\\n    minimized\
        \ = True\\n\\n    inputs = [\\n        MultilineInput(\\n            name=\"\
        input_value\",\\n            display_name=\"Input Text\",\\n            value=\"\
        \",\\n            info=\"Message to be passed as input.\",\\n            input_types=[],\\\
        n        ),\\n        BoolInput(\\n            name=\"should_store_message\"\
        ,\\n            display_name=\"Store Messages\",\\n            info=\"Store\
        \ the message in the history.\",\\n            value=True,\\n            advanced=True,\\\
        n        ),\\n        DropdownInput(\\n            name=\"sender\",\\n   \
        \         display_name=\"Sender Type\",\\n            options=[MESSAGE_SENDER_AI,\
        \ MESSAGE_SENDER_USER],\\n            value=MESSAGE_SENDER_USER,\\n      \
        \      info=\"Type of sender.\",\\n            advanced=True,\\n        ),\\\
        n        MessageTextInput(\\n            name=\"sender_name\",\\n        \
        \    display_name=\"Sender Name\",\\n            info=\"Name of the sender.\"\
        ,\\n            value=MESSAGE_SENDER_NAME_USER,\\n            advanced=True,\\\
        n        ),\\n        MessageTextInput(\\n            name=\"session_id\"\
        ,\\n            display_name=\"Session ID\",\\n            info=\"The session\
        \ ID of the chat. If empty, the current session ID parameter will be used.\"\
        ,\\n            advanced=True,\\n        ),\\n        FileInput(\\n      \
        \      name=\"files\",\\n            display_name=\"Files\",\\n          \
        \  file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\\n            info=\"Files\
        \ to be sent with the message.\",\\n            advanced=True,\\n        \
        \    is_list=True,\\n            temp_file=True,\\n        ),\\n        MessageTextInput(\\\
        n            name=\"background_color\",\\n            display_name=\"Background\
        \ Color\",\\n            info=\"The background color of the icon.\",\\n  \
        \          advanced=True,\\n        ),\\n        MessageTextInput(\\n    \
        \        name=\"chat_icon\",\\n            display_name=\"Icon\",\\n     \
        \       info=\"The icon of the message.\",\\n            advanced=True,\\\
        n        ),\\n        MessageTextInput(\\n            name=\"text_color\"\
        ,\\n            display_name=\"Text Color\",\\n            info=\"The text\
        \ color of the name\",\\n            advanced=True,\\n        ),\\n    ]\\\
        n    outputs = [\\n        Output(display_name=\"Chat Message\", name=\"message\"\
        , method=\"message_response\"),\\n    ]\\n\\n    async def message_response(self)\
        \ -> Message:\\n        background_color = self.background_color\\n      \
        \  text_color = self.text_color\\n        icon = self.chat_icon\\n\\n    \
        \    message = await Message.create(\\n            text=self.input_value,\\\
        n            sender=self.sender,\\n            sender_name=self.sender_name,\\\
        n            session_id=self.session_id,\\n            files=self.files,\\\
        n            properties={\\n                \"background_color\": background_color,\\\
        n                \"text_color\": text_color,\\n                \"icon\": icon,\\\
        n            },\\n        )\\n        if self.session_id and isinstance(message,\
        \ Message) and self.should_store_message:\\n            stored_message = await\
        \ self.send_message(\\n                message,\\n            )\\n       \
        \     self.message.value = stored_message\\n            message = stored_message\\\
        n\\n        self.status = message\\n        return message\\n'}, 'files':\
        \ {'advanced': True, 'display_name': 'Files', 'dynamic': False, 'fileTypes':\
        \ ['txt', 'md', 'mdx', 'csv', 'json', 'yaml', 'yml', 'xml', 'html', 'htm',\
        \ 'pdf', 'docx', 'py', 'sh', 'sql', 'js', 'ts', 'tsx', 'jpg', 'jpeg', 'png',\
        \ 'bmp', 'image'], 'file_path': '', 'info': 'Files to be sent with the message.',\
        \ 'list': True, 'name': 'files', 'placeholder': '', 'required': False, 'show':\
        \ True, 'temp_file': True, 'title_case': False, 'trace_as_metadata': True,\
        \ 'type': 'file', 'value': ''}, 'input_value': {'advanced': False, 'display_name':\
        \ 'Input Text', 'dynamic': False, 'info': 'Message to be passed as input.',\
        \ 'input_types': [], 'list': False, 'load_from_db': False, 'multiline': True,\
        \ 'name': 'input_value', 'placeholder': '', 'required': False, 'show': True,\
        \ 'title_case': False, 'trace_as_input': True, 'trace_as_metadata': True,\
        \ 'type': 'str', 'value': 'Hello'}, 'sender': {'advanced': True, 'display_name':\
        \ 'Sender Type', 'dynamic': False, 'info': 'Type of sender.', 'name': 'sender',\
        \ 'options': ['Machine', 'User'], 'placeholder': '', 'required': False, 'show':\
        \ True, 'title_case': False, 'trace_as_metadata': True, 'type': 'str', 'value':\
        \ 'User'}, 'sender_name': {'advanced': True, 'display_name': 'Sender Name',\
        \ 'dynamic': False, 'info': 'Name of the sender.', 'input_types': ['Message'],\
        \ 'list': False, 'load_from_db': False, 'name': 'sender_name', 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'trace_as_input':\
        \ True, 'trace_as_metadata': True, 'type': 'str', 'value': 'User'}, 'session_id':\
        \ {'advanced': True, 'display_name': 'Session ID', 'dynamic': False, 'info':\
        \ 'The session ID of the chat. If empty, the current session ID parameter\
        \ will be used.', 'input_types': ['Message'], 'list': False, 'load_from_db':\
        \ False, 'name': 'session_id', 'placeholder': '', 'required': False, 'show':\
        \ True, 'title_case': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': ''}, 'should_store_message': {'_input_type':\
        \ 'BoolInput', 'advanced': True, 'display_name': 'Store Messages', 'dynamic':\
        \ False, 'info': 'Store the message in the history.', 'list': False, 'name':\
        \ 'should_store_message', 'placeholder': '', 'required': False, 'show': True,\
        \ 'title_case': False, 'trace_as_metadata': True, 'type': 'bool', 'value':\
        \ True}, 'text_color': {'_input_type': 'MessageTextInput', 'advanced': True,\
        \ 'display_name': 'Text Color', 'dynamic': False, 'info': 'The text color\
        \ of the name', 'input_types': ['Message'], 'list': False, 'load_from_db':\
        \ False, 'name': 'text_color', 'placeholder': '', 'required': False, 'show':\
        \ True, 'title_case': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': ''}}\n\n# Extract runtime inputs (from other\
        \ workflow steps)\nruntime_inputs = {}\nif isinstance(input, dict):\n    for\
        \ key, value in input.items():\n        if key != '_stepflow_meta':  # Skip\
        \ internal metadata\n            runtime_inputs[key] = value\n\n# Call the\
        \ UDF executor with the component configuration\nresult = await context.call_component(\n\
        \    \"langflow://udf_executor\",\n    {\n        \"code\": \"\"\"\nfrom langflow.base.data.utils\
        \ import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import\
        \ ChatComponent\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io\
        \ import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n  \
        \  MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\n\
        from langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n\
        \    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name\
        \ = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\
        \n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\
        \n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized =\
        \ True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\"\
        ,\n            display_name=\"Input Text\",\n            value=\"\",\n   \
        \         info=\"Message to be passed as input.\",\n            input_types=[],\n\
        \        ),\n        BoolInput(\n            name=\"should_store_message\"\
        ,\n            display_name=\"Store Messages\",\n            info=\"Store\
        \ the message in the history.\",\n            value=True,\n            advanced=True,\n\
        \        ),\n        DropdownInput(\n            name=\"sender\",\n      \
        \      display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI,\
        \ MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n        \
        \    info=\"Type of sender.\",\n            advanced=True,\n        ),\n \
        \       MessageTextInput(\n            name=\"sender_name\",\n           \
        \ display_name=\"Sender Name\",\n            info=\"Name of the sender.\"\
        ,\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n\
        \        ),\n        MessageTextInput(\n            name=\"session_id\",\n\
        \            display_name=\"Session ID\",\n            info=\"The session\
        \ ID of the chat. If empty, the current session ID parameter will be used.\"\
        ,\n            advanced=True,\n        ),\n        FileInput(\n          \
        \  name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES\
        \ + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\"\
        ,\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n\
        \        ),\n        MessageTextInput(\n            name=\"background_color\"\
        ,\n            display_name=\"Background Color\",\n            info=\"The\
        \ background color of the icon.\",\n            advanced=True,\n        ),\n\
        \        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"\
        Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n\
        \        ),\n        MessageTextInput(\n            name=\"text_color\",\n\
        \            display_name=\"Text Color\",\n            info=\"The text color\
        \ of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs\
        \ = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"\
        message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n\
        \        background_color = self.background_color\n        text_color = self.text_color\n\
        \        icon = self.chat_icon\n\n        message = await Message.create(\n\
        \            text=self.input_value,\n            sender=self.sender,\n   \
        \         sender_name=self.sender_name,\n            session_id=self.session_id,\n\
        \            files=self.files,\n            properties={\n               \
        \ \"background_color\": background_color,\n                \"text_color\"\
        : text_color,\n                \"icon\": icon,\n            },\n        )\n\
        \        if self.session_id and isinstance(message, Message) and self.should_store_message:\n\
        \            stored_message = await self.send_message(\n                message,\n\
        \            )\n            self.message.value = stored_message\n        \
        \    message = stored_message\n\n        self.status = message\n        return\
        \ message\n\n\"\"\",\n        \"template\": template_config,\n        \"component_type\"\
        : \"ChatInput\",\n        \"runtime_inputs\": runtime_inputs\n    }\n)\n\n\
        # Return the result\nresult"
      function_name: null
      component_type: ChatInput
      display_name: Chat Input
      description: Get chat inputs from the Playground.
- id: create_Prompt-cxvt9_udf
  component: builtin://put_blob
  input:
    data:
      input_schema:
        type: object
        properties:
          code:
            type: string
            description: ''
          template:
            type: string
            description: ''
          tool_placeholder:
            type: string
            description: A placeholder input for tool mode.
        required:
        - code
      code: "# Auto-generated UDF code for Prompt\n# This calls the langflow_component_server_udf_only.py\
        \ udf_executor\n\n# Extract the component's template configuration\ntemplate_config\
        \ = {'_type': 'Component', 'code': {'advanced': True, 'dynamic': True, 'fileTypes':\
        \ [], 'file_path': '', 'info': '', 'list': False, 'load_from_db': False, 'multiline':\
        \ True, 'name': 'code', 'password': False, 'placeholder': '', 'required':\
        \ True, 'show': True, 'title_case': False, 'type': 'code', 'value': 'from\
        \ langflow.base.prompts.api_utils import process_prompt_template\\nfrom langflow.custom.custom_component.component\
        \ import Component\\nfrom langflow.inputs.inputs import DefaultPromptField\\\
        nfrom langflow.io import MessageTextInput, Output, PromptInput\\nfrom langflow.schema.message\
        \ import Message\\nfrom langflow.template.utils import update_template_values\\\
        n\\n\\nclass PromptComponent(Component):\\n    display_name: str = \"Prompt\"\
        \\n    description: str = \"Create a prompt template with dynamic variables.\"\
        \\n    icon = \"braces\"\\n    trace_type = \"prompt\"\\n    name = \"Prompt\"\
        \\n\\n    inputs = [\\n        PromptInput(name=\"template\", display_name=\"\
        Template\"),\\n        MessageTextInput(\\n            name=\"tool_placeholder\"\
        ,\\n            display_name=\"Tool Placeholder\",\\n            tool_mode=True,\\\
        n            advanced=True,\\n            info=\"A placeholder input for tool\
        \ mode.\",\\n        ),\\n    ]\\n\\n    outputs = [\\n        Output(display_name=\"\
        Prompt\", name=\"prompt\", method=\"build_prompt\"),\\n    ]\\n\\n    async\
        \ def build_prompt(self) -> Message:\\n        prompt = Message.from_template(**self._attributes)\\\
        n        self.status = prompt.text\\n        return prompt\\n\\n    def _update_template(self,\
        \ frontend_node: dict):\\n        prompt_template = frontend_node[\"template\"\
        ][\"template\"][\"value\"]\\n        custom_fields = frontend_node[\"custom_fields\"\
        ]\\n        frontend_node_template = frontend_node[\"template\"]\\n      \
        \  _ = process_prompt_template(\\n            template=prompt_template,\\\
        n            name=\"template\",\\n            custom_fields=custom_fields,\\\
        n            frontend_node_template=frontend_node_template,\\n        )\\\
        n        return frontend_node\\n\\n    async def update_frontend_node(self,\
        \ new_frontend_node: dict, current_frontend_node: dict):\\n        \"\"\"\
        This function is called after the code validation is done.\"\"\"\\n      \
        \  frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\\\
        n        template = frontend_node[\"template\"][\"template\"][\"value\"]\\\
        n        # Kept it duplicated for backwards compatibility\\n        _ = process_prompt_template(\\\
        n            template=template,\\n            name=\"template\",\\n      \
        \      custom_fields=frontend_node[\"custom_fields\"],\\n            frontend_node_template=frontend_node[\"\
        template\"],\\n        )\\n        # Now that template is updated, we need\
        \ to grab any values that were set in the current_frontend_node\\n       \
        \ # and update the frontend_node with those values\\n        update_template_values(new_template=frontend_node,\
        \ previous_template=current_frontend_node[\"template\"])\\n        return\
        \ frontend_node\\n\\n    def _get_fallback_input(self, **kwargs):\\n     \
        \   return DefaultPromptField(**kwargs)\\n'}, 'template': {'_input_type':\
        \ 'PromptInput', 'advanced': False, 'display_name': 'Template', 'dynamic':\
        \ False, 'info': '', 'list': False, 'load_from_db': False, 'name': 'template',\
        \ 'placeholder': '', 'required': False, 'show': True, 'title_case': False,\
        \ 'tool_mode': False, 'trace_as_input': True, 'type': 'prompt', 'value': 'Answer\
        \ the user as if you were a GenAI expert, enthusiastic about helping them\
        \ get started building something fresh.'}, 'tool_placeholder': {'_input_type':\
        \ 'MessageTextInput', 'advanced': True, 'display_name': 'Tool Placeholder',\
        \ 'dynamic': False, 'info': 'A placeholder input for tool mode.', 'input_types':\
        \ ['Message'], 'list': False, 'load_from_db': False, 'name': 'tool_placeholder',\
        \ 'placeholder': '', 'required': False, 'show': True, 'title_case': False,\
        \ 'tool_mode': True, 'trace_as_input': True, 'trace_as_metadata': True, 'type':\
        \ 'str', 'value': ''}}\n\n# Extract runtime inputs (from other workflow steps)\n\
        runtime_inputs = {}\nif isinstance(input, dict):\n    for key, value in input.items():\n\
        \        if key != '_stepflow_meta':  # Skip internal metadata\n         \
        \   runtime_inputs[key] = value\n\n# Call the UDF executor with the component\
        \ configuration\nresult = await context.call_component(\n    \"langflow://udf_executor\"\
        ,\n    {\n        \"code\": \"\"\"\nfrom langflow.base.prompts.api_utils import\
        \ process_prompt_template\nfrom langflow.custom.custom_component.component\
        \ import Component\nfrom langflow.inputs.inputs import DefaultPromptField\n\
        from langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message\
        \ import Message\nfrom langflow.template.utils import update_template_values\n\
        \n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n\
        \    description: str = \"Create a prompt template with dynamic variables.\"\
        \n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\
        \n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"\
        Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\"\
        ,\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n\
        \            advanced=True,\n            info=\"A placeholder input for tool\
        \ mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"\
        Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def\
        \ build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n\
        \        self.status = prompt.text\n        return prompt\n\n    def _update_template(self,\
        \ frontend_node: dict):\n        prompt_template = frontend_node[\"template\"\
        ][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"\
        ]\n        frontend_node_template = frontend_node[\"template\"]\n        _\
        \ = process_prompt_template(\n            template=prompt_template,\n    \
        \        name=\"template\",\n            custom_fields=custom_fields,\n  \
        \          frontend_node_template=frontend_node_template,\n        )\n   \
        \     return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node:\
        \ dict, current_frontend_node: dict):\n        \"\"\"This function is called\
        \ after the code validation is done.\"\"\"\n        frontend_node = await\
        \ super().update_frontend_node(new_frontend_node, current_frontend_node)\n\
        \        template = frontend_node[\"template\"][\"template\"][\"value\"]\n\
        \        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n\
        \            template=template,\n            name=\"template\",\n        \
        \    custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"\
        template\"],\n        )\n        # Now that template is updated, we need to\
        \ grab any values that were set in the current_frontend_node\n        # and\
        \ update the frontend_node with those values\n        update_template_values(new_template=frontend_node,\
        \ previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\
        \n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n\
        \n\"\"\",\n        \"template\": template_config,\n        \"component_type\"\
        : \"Prompt\",\n        \"runtime_inputs\": runtime_inputs\n    }\n)\n\n# Return\
        \ the result\nresult"
      function_name: null
      component_type: Prompt
      display_name: Prompt
      description: Create a prompt template with dynamic variables.
- id: create_undefined-JnoW8_udf
  component: builtin://put_blob
  input:
    data:
      input_schema:
        type: object
        properties: {}
        required: []
      code: "# Auto-generated UDF code for unknown\n# This calls the langflow_component_server_udf_only.py\
        \ udf_executor\n\n# Extract the component's template configuration\ntemplate_config\
        \ = {'backgroundColor': 'neutral'}\n\n# Extract runtime inputs (from other\
        \ workflow steps)\nruntime_inputs = {}\nif isinstance(input, dict):\n    for\
        \ key, value in input.items():\n        if key != '_stepflow_meta':  # Skip\
        \ internal metadata\n            runtime_inputs[key] = value\n\n# Call the\
        \ UDF executor with the component configuration\nresult = await context.call_component(\n\
        \    \"langflow://udf_executor\",\n    {\n        \"code\": \"\"\"\n\n\"\"\
        \",\n        \"template\": template_config,\n        \"component_type\": \"\
        unknown\",\n        \"runtime_inputs\": runtime_inputs\n    }\n)\n\n# Return\
        \ the result\nresult"
      function_name: null
      component_type: unknown
      display_name: Read Me
      description: "## \U0001F4D6 README\n\nPerform basic prompting with a Language\
        \ model component.\n\n#### Quick Start\n- Add your **OpenAI API key** to the\
        \ **Language Model** component.\n- Open the **Playground** to chat with your\
        \ bot.\n\n#### Next steps:\nExperiment by changing the prompt and the Language\
        \ model temperature to see how the bot's responses change."
- id: create_ChatOutput-bKrF6_udf
  component: builtin://put_blob
  input:
    data:
      input_schema:
        type: object
        properties:
          background_color:
            type: string
            description: The background color of the icon.
          chat_icon:
            type: string
            description: The icon of the message.
          clean_data:
            type: boolean
            description: Whether to clean the data
          code:
            type: string
            description: ''
          data_template:
            type: string
            description: Template to convert Data to Text. If left empty, it will
              be dynamically set to the Data's text key.
          input_value:
            type: string
            description: Message to be passed as output.
          sender:
            type: string
            description: Type of sender.
          sender_name:
            type: string
            description: Name of the sender.
          session_id:
            type: string
            description: The session ID of the chat. If empty, the current session
              ID parameter will be used.
          should_store_message:
            type: boolean
            description: Store the message in the history.
          text_color:
            type: string
            description: The text color of the name
        required:
        - code
        - input_value
      code: "# Auto-generated UDF code for ChatOutput\n# This calls the langflow_component_server_udf_only.py\
        \ udf_executor\n\n# Extract the component's template configuration\ntemplate_config\
        \ = {'_type': 'Component', 'background_color': {'_input_type': 'MessageTextInput',\
        \ 'advanced': True, 'display_name': 'Background Color', 'dynamic': False,\
        \ 'info': 'The background color of the icon.', 'input_types': ['Message'],\
        \ 'list': False, 'load_from_db': False, 'name': 'background_color', 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'tool_mode': False,\
        \ 'trace_as_input': True, 'trace_as_metadata': True, 'type': 'str', 'value':\
        \ ''}, 'chat_icon': {'_input_type': 'MessageTextInput', 'advanced': True,\
        \ 'display_name': 'Icon', 'dynamic': False, 'info': 'The icon of the message.',\
        \ 'input_types': ['Message'], 'list': False, 'load_from_db': False, 'name':\
        \ 'chat_icon', 'placeholder': '', 'required': False, 'show': True, 'title_case':\
        \ False, 'tool_mode': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': ''}, 'clean_data': {'_input_type': 'BoolInput',\
        \ 'advanced': True, 'display_name': 'Basic Clean Data', 'dynamic': False,\
        \ 'info': 'Whether to clean the data', 'list': False, 'list_add_label': 'Add\
        \ More', 'name': 'clean_data', 'placeholder': '', 'required': False, 'show':\
        \ True, 'title_case': False, 'tool_mode': False, 'trace_as_metadata': True,\
        \ 'type': 'bool', 'value': True}, 'code': {'advanced': True, 'dynamic': True,\
        \ 'fileTypes': [], 'file_path': '', 'info': '', 'list': False, 'load_from_db':\
        \ False, 'multiline': True, 'name': 'code', 'password': False, 'placeholder':\
        \ '', 'required': True, 'show': True, 'title_case': False, 'type': 'code',\
        \ 'value': 'from collections.abc import Generator\\nfrom typing import Any\\\
        n\\nimport orjson\\nfrom fastapi.encoders import jsonable_encoder\\n\\nfrom\
        \ langflow.base.io.chat import ChatComponent\\nfrom langflow.helpers.data\
        \ import safe_convert\\nfrom langflow.inputs.inputs import BoolInput, DropdownInput,\
        \ HandleInput, MessageTextInput\\nfrom langflow.schema.data import Data\\\
        nfrom langflow.schema.dataframe import DataFrame\\nfrom langflow.schema.message\
        \ import Message\\nfrom langflow.schema.properties import Source\\nfrom langflow.template.field.base\
        \ import Output\\nfrom langflow.utils.constants import (\\n    MESSAGE_SENDER_AI,\\\
        n    MESSAGE_SENDER_NAME_AI,\\n    MESSAGE_SENDER_USER,\\n)\\n\\n\\nclass\
        \ ChatOutput(ChatComponent):\\n    display_name = \"Chat Output\"\\n    description\
        \ = \"Display a chat message in the Playground.\"\\n    documentation: str\
        \ = \"https://docs.langflow.org/components-io#chat-output\"\\n    icon = \"\
        MessagesSquare\"\\n    name = \"ChatOutput\"\\n    minimized = True\\n\\n\
        \    inputs = [\\n        HandleInput(\\n            name=\"input_value\"\
        ,\\n            display_name=\"Inputs\",\\n            info=\"Message to be\
        \ passed as output.\",\\n            input_types=[\"Data\", \"DataFrame\"\
        , \"Message\"],\\n            required=True,\\n        ),\\n        BoolInput(\\\
        n            name=\"should_store_message\",\\n            display_name=\"\
        Store Messages\",\\n            info=\"Store the message in the history.\"\
        ,\\n            value=True,\\n            advanced=True,\\n        ),\\n \
        \       DropdownInput(\\n            name=\"sender\",\\n            display_name=\"\
        Sender Type\",\\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\\\
        n            value=MESSAGE_SENDER_AI,\\n            advanced=True,\\n    \
        \        info=\"Type of sender.\",\\n        ),\\n        MessageTextInput(\\\
        n            name=\"sender_name\",\\n            display_name=\"Sender Name\"\
        ,\\n            info=\"Name of the sender.\",\\n            value=MESSAGE_SENDER_NAME_AI,\\\
        n            advanced=True,\\n        ),\\n        MessageTextInput(\\n  \
        \          name=\"session_id\",\\n            display_name=\"Session ID\"\
        ,\\n            info=\"The session ID of the chat. If empty, the current session\
        \ ID parameter will be used.\",\\n            advanced=True,\\n        ),\\\
        n        MessageTextInput(\\n            name=\"data_template\",\\n      \
        \      display_name=\"Data Template\",\\n            value=\"{text}\",\\n\
        \            advanced=True,\\n            info=\"Template to convert Data\
        \ to Text. If left empty, it will be dynamically set to the Data\\'s text\
        \ key.\",\\n        ),\\n        MessageTextInput(\\n            name=\"background_color\"\
        ,\\n            display_name=\"Background Color\",\\n            info=\"The\
        \ background color of the icon.\",\\n            advanced=True,\\n       \
        \ ),\\n        MessageTextInput(\\n            name=\"chat_icon\",\\n    \
        \        display_name=\"Icon\",\\n            info=\"The icon of the message.\"\
        ,\\n            advanced=True,\\n        ),\\n        MessageTextInput(\\\
        n            name=\"text_color\",\\n            display_name=\"Text Color\"\
        ,\\n            info=\"The text color of the name\",\\n            advanced=True,\\\
        n        ),\\n        BoolInput(\\n            name=\"clean_data\",\\n   \
        \         display_name=\"Basic Clean Data\",\\n            value=True,\\n\
        \            info=\"Whether to clean the data\",\\n            advanced=True,\\\
        n        ),\\n    ]\\n    outputs = [\\n        Output(\\n            display_name=\"\
        Output Message\",\\n            name=\"message\",\\n            method=\"\
        message_response\",\\n        ),\\n    ]\\n\\n    def _build_source(self,\
        \ id_: str | None, display_name: str | None, source: str | None) -> Source:\\\
        n        source_dict = {}\\n        if id_:\\n            source_dict[\"id\"\
        ] = id_\\n        if display_name:\\n            source_dict[\"display_name\"\
        ] = display_name\\n        if source:\\n            # Handle case where source\
        \ is a ChatOpenAI object\\n            if hasattr(source, \"model_name\"):\\\
        n                source_dict[\"source\"] = source.model_name\\n          \
        \  elif hasattr(source, \"model\"):\\n                source_dict[\"source\"\
        ] = str(source.model)\\n            else:\\n                source_dict[\"\
        source\"] = str(source)\\n        return Source(**source_dict)\\n\\n    async\
        \ def message_response(self) -> Message:\\n        # First convert the input\
        \ to string if needed\\n        text = self.convert_to_string()\\n\\n    \
        \    # Get source properties\\n        source, icon, display_name, source_id\
        \ = self.get_properties_from_source_component()\\n        background_color\
        \ = self.background_color\\n        text_color = self.text_color\\n      \
        \  if self.chat_icon:\\n            icon = self.chat_icon\\n\\n        # Create\
        \ or use existing Message object\\n        if isinstance(self.input_value,\
        \ Message):\\n            message = self.input_value\\n            # Update\
        \ message properties\\n            message.text = text\\n        else:\\n\
        \            message = Message(text=text)\\n\\n        # Set message properties\\\
        n        message.sender = self.sender\\n        message.sender_name = self.sender_name\\\
        n        message.session_id = self.session_id\\n        message.flow_id =\
        \ self.graph.flow_id if hasattr(self, \"graph\") else None\\n        message.properties.source\
        \ = self._build_source(source_id, display_name, source)\\n        message.properties.icon\
        \ = icon\\n        message.properties.background_color = background_color\\\
        n        message.properties.text_color = text_color\\n\\n        # Store message\
        \ if needed\\n        if self.session_id and self.should_store_message:\\\
        n            stored_message = await self.send_message(message)\\n        \
        \    self.message.value = stored_message\\n            message = stored_message\\\
        n\\n        self.status = message\\n        return message\\n\\n    def _serialize_data(self,\
        \ data: Data) -> str:\\n        \"\"\"Serialize Data object to JSON string.\"\
        \"\"\\n        # Convert data.data to JSON-serializable format\\n        serializable_data\
        \ = jsonable_encoder(data.data)\\n        # Serialize with orjson, enabling\
        \ pretty printing with indentation\\n        json_bytes = orjson.dumps(serializable_data,\
        \ option=orjson.OPT_INDENT_2)\\n        # Convert bytes to string and wrap\
        \ in Markdown code blocks\\n        return \"```json\\\\n\" + json_bytes.decode(\"\
        utf-8\") + \"\\\\n```\"\\n\\n    def _validate_input(self) -> None:\\n   \
        \     \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\
        \\n        if self.input_value is None:\\n            msg = \"Input data cannot\
        \ be None\"\\n            raise ValueError(msg)\\n        if isinstance(self.input_value,\
        \ list) and not all(\\n            isinstance(item, Message | Data | DataFrame\
        \ | str) for item in self.input_value\\n        ):\\n            invalid_types\
        \ = [\\n                type(item).__name__\\n                for item in\
        \ self.input_value\\n                if not isinstance(item, Message | Data\
        \ | DataFrame | str)\\n            ]\\n            msg = f\"Expected Data\
        \ or DataFrame or Message or str, got {invalid_types}\"\\n            raise\
        \ TypeError(msg)\\n        if not isinstance(\\n            self.input_value,\\\
        n            Message | Data | DataFrame | str | list | Generator | type(None),\\\
        n        ):\\n            type_name = type(self.input_value).__name__\\n \
        \           msg = f\"Expected Data or DataFrame or Message or str, Generator\
        \ or None, got {type_name}\"\\n            raise TypeError(msg)\\n\\n    def\
        \ convert_to_string(self) -> str | Generator[Any, None, None]:\\n        \"\
        \"\"Convert input data to string with proper error handling.\"\"\"\\n    \
        \    self._validate_input()\\n        if isinstance(self.input_value, list):\\\
        n            return \"\\\\n\".join([safe_convert(item, clean_data=self.clean_data)\
        \ for item in self.input_value])\\n        if isinstance(self.input_value,\
        \ Generator):\\n            return self.input_value\\n        return safe_convert(self.input_value)\\\
        n'}, 'data_template': {'_input_type': 'MessageTextInput', 'advanced': True,\
        \ 'display_name': 'Data Template', 'dynamic': False, 'info': \"Template to\
        \ convert Data to Text. If left empty, it will be dynamically set to the Data's\
        \ text key.\", 'input_types': ['Message'], 'list': False, 'load_from_db':\
        \ False, 'name': 'data_template', 'placeholder': '', 'required': False, 'show':\
        \ True, 'title_case': False, 'tool_mode': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': '{text}'}, 'input_value': {'_input_type':\
        \ 'MessageInput', 'advanced': False, 'display_name': 'Inputs', 'dynamic':\
        \ False, 'info': 'Message to be passed as output.', 'input_types': ['Data',\
        \ 'DataFrame', 'Message'], 'list': False, 'load_from_db': False, 'name': 'input_value',\
        \ 'placeholder': '', 'required': True, 'show': True, 'title_case': False,\
        \ 'trace_as_input': True, 'trace_as_metadata': True, 'type': 'str', 'value':\
        \ ''}, 'sender': {'_input_type': 'DropdownInput', 'advanced': True, 'combobox':\
        \ False, 'display_name': 'Sender Type', 'dynamic': False, 'info': 'Type of\
        \ sender.', 'name': 'sender', 'options': ['Machine', 'User'], 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'tool_mode': False,\
        \ 'trace_as_metadata': True, 'type': 'str', 'value': 'Machine'}, 'sender_name':\
        \ {'_input_type': 'MessageTextInput', 'advanced': True, 'display_name': 'Sender\
        \ Name', 'dynamic': False, 'info': 'Name of the sender.', 'input_types': ['Message'],\
        \ 'list': False, 'load_from_db': False, 'name': 'sender_name', 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'tool_mode': False,\
        \ 'trace_as_input': True, 'trace_as_metadata': True, 'type': 'str', 'value':\
        \ 'AI'}, 'session_id': {'_input_type': 'MessageTextInput', 'advanced': True,\
        \ 'display_name': 'Session ID', 'dynamic': False, 'info': 'The session ID\
        \ of the chat. If empty, the current session ID parameter will be used.',\
        \ 'input_types': ['Message'], 'list': False, 'load_from_db': False, 'name':\
        \ 'session_id', 'placeholder': '', 'required': False, 'show': True, 'title_case':\
        \ False, 'tool_mode': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': ''}, 'should_store_message': {'_input_type':\
        \ 'BoolInput', 'advanced': True, 'display_name': 'Store Messages', 'dynamic':\
        \ False, 'info': 'Store the message in the history.', 'list': False, 'name':\
        \ 'should_store_message', 'placeholder': '', 'required': False, 'show': True,\
        \ 'title_case': False, 'trace_as_metadata': True, 'type': 'bool', 'value':\
        \ True}, 'text_color': {'_input_type': 'MessageTextInput', 'advanced': True,\
        \ 'display_name': 'Text Color', 'dynamic': False, 'info': 'The text color\
        \ of the name', 'input_types': ['Message'], 'list': False, 'load_from_db':\
        \ False, 'name': 'text_color', 'placeholder': '', 'required': False, 'show':\
        \ True, 'title_case': False, 'tool_mode': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': ''}}\n\n# Extract runtime inputs (from other\
        \ workflow steps)\nruntime_inputs = {}\nif isinstance(input, dict):\n    for\
        \ key, value in input.items():\n        if key != '_stepflow_meta':  # Skip\
        \ internal metadata\n            runtime_inputs[key] = value\n\n# Call the\
        \ UDF executor with the component configuration\nresult = await context.call_component(\n\
        \    \"langflow://udf_executor\",\n    {\n        \"code\": \"\"\"\nfrom collections.abc\
        \ import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders\
        \ import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\n\
        from langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs\
        \ import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data\
        \ import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message\
        \ import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base\
        \ import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n\
        \    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n\
        \    display_name = \"Chat Output\"\n    description = \"Display a chat message\
        \ in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\
        \n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized\
        \ = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\"\
        ,\n            display_name=\"Inputs\",\n            info=\"Message to be\
        \ passed as output.\",\n            input_types=[\"Data\", \"DataFrame\",\
        \ \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n\
        \            name=\"should_store_message\",\n            display_name=\"Store\
        \ Messages\",\n            info=\"Store the message in the history.\",\n \
        \           value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n\
        \            name=\"sender\",\n            display_name=\"Sender Type\",\n\
        \            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n         \
        \   value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"\
        Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"\
        sender_name\",\n            display_name=\"Sender Name\",\n            info=\"\
        Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n      \
        \      advanced=True,\n        ),\n        MessageTextInput(\n           \
        \ name=\"session_id\",\n            display_name=\"Session ID\",\n       \
        \     info=\"The session ID of the chat. If empty, the current session ID\
        \ parameter will be used.\",\n            advanced=True,\n        ),\n   \
        \     MessageTextInput(\n            name=\"data_template\",\n           \
        \ display_name=\"Data Template\",\n            value=\"{text}\",\n       \
        \     advanced=True,\n            info=\"Template to convert Data to Text.\
        \ If left empty, it will be dynamically set to the Data's text key.\",\n \
        \       ),\n        MessageTextInput(\n            name=\"background_color\"\
        ,\n            display_name=\"Background Color\",\n            info=\"The\
        \ background color of the icon.\",\n            advanced=True,\n        ),\n\
        \        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"\
        Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n\
        \        ),\n        MessageTextInput(\n            name=\"text_color\",\n\
        \            display_name=\"Text Color\",\n            info=\"The text color\
        \ of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n\
        \            name=\"clean_data\",\n            display_name=\"Basic Clean\
        \ Data\",\n            value=True,\n            info=\"Whether to clean the\
        \ data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n\
        \        Output(\n            display_name=\"Output Message\",\n         \
        \   name=\"message\",\n            method=\"message_response\",\n        ),\n\
        \    ]\n\n    def _build_source(self, id_: str | None, display_name: str |\
        \ None, source: str | None) -> Source:\n        source_dict = {}\n       \
        \ if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n\
        \            source_dict[\"display_name\"] = display_name\n        if source:\n\
        \            # Handle case where source is a ChatOpenAI object\n         \
        \   if hasattr(source, \"model_name\"):\n                source_dict[\"source\"\
        ] = source.model_name\n            elif hasattr(source, \"model\"):\n    \
        \            source_dict[\"source\"] = str(source.model)\n            else:\n\
        \                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\
        \n    async def message_response(self) -> Message:\n        # First convert\
        \ the input to string if needed\n        text = self.convert_to_string()\n\
        \n        # Get source properties\n        source, icon, display_name, source_id\
        \ = self.get_properties_from_source_component()\n        background_color\
        \ = self.background_color\n        text_color = self.text_color\n        if\
        \ self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or\
        \ use existing Message object\n        if isinstance(self.input_value, Message):\n\
        \            message = self.input_value\n            # Update message properties\n\
        \            message.text = text\n        else:\n            message = Message(text=text)\n\
        \n        # Set message properties\n        message.sender = self.sender\n\
        \        message.sender_name = self.sender_name\n        message.session_id\
        \ = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self,\
        \ \"graph\") else None\n        message.properties.source = self._build_source(source_id,\
        \ display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color\
        \ = background_color\n        message.properties.text_color = text_color\n\
        \n        # Store message if needed\n        if self.session_id and self.should_store_message:\n\
        \            stored_message = await self.send_message(message)\n         \
        \   self.message.value = stored_message\n            message = stored_message\n\
        \n        self.status = message\n        return message\n\n    def _serialize_data(self,\
        \ data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\
        \"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data\
        \ = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling\
        \ pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data,\
        \ option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap\
        \ in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"\
        utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\
        \"\"Validate the input data and raise ValueError if invalid.\"\"\"\n     \
        \   if self.input_value is None:\n            msg = \"Input data cannot be\
        \ None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value,\
        \ list) and not all(\n            isinstance(item, Message | Data | DataFrame\
        \ | str) for item in self.input_value\n        ):\n            invalid_types\
        \ = [\n                type(item).__name__\n                for item in self.input_value\n\
        \                if not isinstance(item, Message | Data | DataFrame | str)\n\
        \            ]\n            msg = f\"Expected Data or DataFrame or Message\
        \ or str, got {invalid_types}\"\n            raise TypeError(msg)\n      \
        \  if not isinstance(\n            self.input_value,\n            Message\
        \ | Data | DataFrame | str | list | Generator | type(None),\n        ):\n\
        \            type_name = type(self.input_value).__name__\n            msg\
        \ = f\"Expected Data or DataFrame or Message or str, Generator or None, got\
        \ {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self)\
        \ -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to\
        \ string with proper error handling.\"\"\"\n        self._validate_input()\n\
        \        if isinstance(self.input_value, list):\n            return \"\\n\"\
        .join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n\
        \        if isinstance(self.input_value, Generator):\n            return self.input_value\n\
        \        return safe_convert(self.input_value)\n\n\"\"\",\n        \"template\"\
        : template_config,\n        \"component_type\": \"ChatOutput\",\n        \"\
        runtime_inputs\": runtime_inputs\n    }\n)\n\n# Return the result\nresult"
      function_name: null
      component_type: ChatOutput
      display_name: Chat Output
      description: Display a chat message in the Playground.
  depends_on:
  - create_LanguageModelComponent-TsLa4_udf
- id: create_LanguageModelComponent-TsLa4_udf
  component: builtin://put_blob
  input:
    data:
      input_schema:
        type: object
        properties:
          api_key:
            type: string
            description: Model Provider API key
          code:
            type: string
            description: ''
          input_value:
            type: string
            description: The input text to send to the model
          model_name:
            type: string
            description: Select the model to use
          provider:
            type: string
            description: Select the model provider
          stream:
            type: boolean
            description: Whether to stream the response
          system_message:
            type: string
            description: A system message that helps set the behavior of the assistant
          temperature:
            type: number
            description: Controls randomness in responses
            minimum: 0
            maximum: 1
        required:
        - code
      code: "# Auto-generated UDF code for LanguageModelComponent\n# This calls the\
        \ langflow_component_server_udf_only.py udf_executor\n\n# Extract the component's\
        \ template configuration\ntemplate_config = {'_type': 'Component', 'api_key':\
        \ {'_input_type': 'SecretStrInput', 'advanced': False, 'display_name': 'OpenAI\
        \ API Key', 'dynamic': False, 'info': 'Model Provider API key', 'input_types':\
        \ [], 'load_from_db': True, 'name': 'api_key', 'password': True, 'placeholder':\
        \ '', 'real_time_refresh': True, 'required': False, 'show': True, 'title_case':\
        \ False, 'type': 'str', 'value': 'OPENAI_API_KEY'}, 'code': {'advanced': True,\
        \ 'dynamic': True, 'fileTypes': [], 'file_path': '', 'info': '', 'list': False,\
        \ 'load_from_db': False, 'multiline': True, 'name': 'code', 'password': False,\
        \ 'placeholder': '', 'required': True, 'show': True, 'title_case': False,\
        \ 'type': 'code', 'value': 'from typing import Any\\n\\nfrom langchain_anthropic\
        \ import ChatAnthropic\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\\
        nfrom langchain_openai import ChatOpenAI\\n\\nfrom langflow.base.models.anthropic_constants\
        \ import ANTHROPIC_MODELS\\nfrom langflow.base.models.google_generative_ai_constants\
        \ import GOOGLE_GENERATIVE_AI_MODELS\\nfrom langflow.base.models.model import\
        \ LCModelComponent\\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\\\
        nfrom langflow.field_typing import LanguageModel\\nfrom langflow.field_typing.range_spec\
        \ import RangeSpec\\nfrom langflow.inputs.inputs import BoolInput\\nfrom langflow.io\
        \ import DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\\\
        nfrom langflow.schema.dotdict import dotdict\\n\\n\\nclass LanguageModelComponent(LCModelComponent):\\\
        n    display_name = \"Language Model\"\\n    description = \"Runs a language\
        \ model given a specified provider.\"\\n    documentation: str = \"https://docs.langflow.org/components-models\"\
        \\n    icon = \"brain-circuit\"\\n    category = \"models\"\\n    priority\
        \ = 0  # Set priority to 0 to make it appear first\\n\\n    inputs = [\\n\
        \        DropdownInput(\\n            name=\"provider\",\\n            display_name=\"\
        Model Provider\",\\n            options=[\"OpenAI\", \"Anthropic\", \"Google\"\
        ],\\n            value=\"OpenAI\",\\n            info=\"Select the model provider\"\
        ,\\n            real_time_refresh=True,\\n            options_metadata=[{\"\
        icon\": \"OpenAI\"}, {\"icon\": \"Anthropic\"}, {\"icon\": \"GoogleGenerativeAI\"\
        }],\\n        ),\\n        DropdownInput(\\n            name=\"model_name\"\
        ,\\n            display_name=\"Model Name\",\\n            options=OPENAI_MODEL_NAMES,\\\
        n            value=OPENAI_MODEL_NAMES[0],\\n            info=\"Select the\
        \ model to use\",\\n        ),\\n        SecretStrInput(\\n            name=\"\
        api_key\",\\n            display_name=\"OpenAI API Key\",\\n            info=\"\
        Model Provider API key\",\\n            required=False,\\n            show=True,\\\
        n            real_time_refresh=True,\\n        ),\\n        MessageInput(\\\
        n            name=\"input_value\",\\n            display_name=\"Input\",\\\
        n            info=\"The input text to send to the model\",\\n        ),\\\
        n        MultilineInput(\\n            name=\"system_message\",\\n       \
        \     display_name=\"System Message\",\\n            info=\"A system message\
        \ that helps set the behavior of the assistant\",\\n            advanced=True,\\\
        n        ),\\n        BoolInput(\\n            name=\"stream\",\\n       \
        \     display_name=\"Stream\",\\n            info=\"Whether to stream the\
        \ response\",\\n            value=False,\\n            advanced=True,\\n \
        \       ),\\n        SliderInput(\\n            name=\"temperature\",\\n \
        \           display_name=\"Temperature\",\\n            value=0.1,\\n    \
        \        info=\"Controls randomness in responses\",\\n            range_spec=RangeSpec(min=0,\
        \ max=1, step=0.01),\\n            advanced=True,\\n        ),\\n    ]\\n\\\
        n    def build_model(self) -> LanguageModel:\\n        provider = self.provider\\\
        n        model_name = self.model_name\\n        temperature = self.temperature\\\
        n        stream = self.stream\\n\\n        if provider == \"OpenAI\":\\n \
        \           if not self.api_key:\\n                msg = \"OpenAI API key\
        \ is required when using OpenAI provider\"\\n                raise ValueError(msg)\\\
        n            return ChatOpenAI(\\n                model_name=model_name,\\\
        n                temperature=temperature,\\n                streaming=stream,\\\
        n                openai_api_key=self.api_key,\\n            )\\n        if\
        \ provider == \"Anthropic\":\\n            if not self.api_key:\\n       \
        \         msg = \"Anthropic API key is required when using Anthropic provider\"\
        \\n                raise ValueError(msg)\\n            return ChatAnthropic(\\\
        n                model=model_name,\\n                temperature=temperature,\\\
        n                streaming=stream,\\n                anthropic_api_key=self.api_key,\\\
        n            )\\n        if provider == \"Google\":\\n            if not self.api_key:\\\
        n                msg = \"Google API key is required when using Google provider\"\
        \\n                raise ValueError(msg)\\n            return ChatGoogleGenerativeAI(\\\
        n                model=model_name,\\n                temperature=temperature,\\\
        n                streaming=stream,\\n                google_api_key=self.api_key,\\\
        n            )\\n        msg = f\"Unknown provider: {provider}\"\\n      \
        \  raise ValueError(msg)\\n\\n    def update_build_config(self, build_config:\
        \ dotdict, field_value: Any, field_name: str | None = None) -> dotdict:\\\
        n        if field_name == \"provider\":\\n            if field_value == \"\
        OpenAI\":\\n                build_config[\"model_name\"][\"options\"] = OPENAI_MODEL_NAMES\\\
        n                build_config[\"model_name\"][\"value\"] = OPENAI_MODEL_NAMES[0]\\\
        n                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API\
        \ Key\"\\n            elif field_value == \"Anthropic\":\\n              \
        \  build_config[\"model_name\"][\"options\"] = ANTHROPIC_MODELS\\n       \
        \         build_config[\"model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\\\
        n                build_config[\"api_key\"][\"display_name\"] = \"Anthropic\
        \ API Key\"\\n            elif field_value == \"Google\":\\n             \
        \   build_config[\"model_name\"][\"options\"] = GOOGLE_GENERATIVE_AI_MODELS\\\
        n                build_config[\"model_name\"][\"value\"] = GOOGLE_GENERATIVE_AI_MODELS[0]\\\
        n                build_config[\"api_key\"][\"display_name\"] = \"Google API\
        \ Key\"\\n        return build_config\\n'}, 'input_value': {'_input_type':\
        \ 'MessageInput', 'advanced': False, 'display_name': 'Input', 'dynamic': False,\
        \ 'info': 'The input text to send to the model', 'input_types': ['Message'],\
        \ 'list': False, 'list_add_label': 'Add More', 'load_from_db': False, 'name':\
        \ 'input_value', 'placeholder': '', 'required': False, 'show': True, 'title_case':\
        \ False, 'tool_mode': False, 'trace_as_input': True, 'trace_as_metadata':\
        \ True, 'type': 'str', 'value': ''}, 'model_name': {'_input_type': 'DropdownInput',\
        \ 'advanced': False, 'combobox': False, 'dialog_inputs': {}, 'display_name':\
        \ 'Model Name', 'dynamic': False, 'info': 'Select the model to use', 'name':\
        \ 'model_name', 'options': ['gpt-4o-mini', 'gpt-4o', 'gpt-4.1', 'gpt-4.1-mini',\
        \ 'gpt-4.1-nano', 'gpt-4.5-preview', 'gpt-4-turbo', 'gpt-4-turbo-preview',\
        \ 'gpt-4', 'gpt-3.5-turbo'], 'options_metadata': [], 'placeholder': '', 'required':\
        \ False, 'show': True, 'title_case': False, 'toggle': False, 'tool_mode':\
        \ False, 'trace_as_metadata': True, 'type': 'str', 'value': 'gpt-4o-mini'},\
        \ 'provider': {'_input_type': 'DropdownInput', 'advanced': False, 'combobox':\
        \ False, 'dialog_inputs': {}, 'display_name': 'Model Provider', 'dynamic':\
        \ False, 'info': 'Select the model provider', 'name': 'provider', 'options':\
        \ ['OpenAI', 'Anthropic', 'Google'], 'options_metadata': [{'icon': 'OpenAI'},\
        \ {'icon': 'Anthropic'}, {'icon': 'GoogleGenerativeAI'}], 'placeholder': '',\
        \ 'real_time_refresh': True, 'required': False, 'show': True, 'title_case':\
        \ False, 'toggle': False, 'tool_mode': False, 'trace_as_metadata': True, 'type':\
        \ 'str', 'value': 'OpenAI'}, 'stream': {'_input_type': 'BoolInput', 'advanced':\
        \ True, 'display_name': 'Stream', 'dynamic': False, 'info': 'Whether to stream\
        \ the response', 'list': False, 'list_add_label': 'Add More', 'name': 'stream',\
        \ 'placeholder': '', 'required': False, 'show': True, 'title_case': False,\
        \ 'tool_mode': False, 'trace_as_metadata': True, 'type': 'bool', 'value':\
        \ False}, 'system_message': {'_input_type': 'MultilineInput', 'advanced':\
        \ False, 'copy_field': False, 'display_name': 'System Message', 'dynamic':\
        \ False, 'info': 'A system message that helps set the behavior of the assistant',\
        \ 'input_types': ['Message'], 'list': False, 'list_add_label': 'Add More',\
        \ 'load_from_db': False, 'multiline': True, 'name': 'system_message', 'placeholder':\
        \ '', 'required': False, 'show': True, 'title_case': False, 'tool_mode': False,\
        \ 'trace_as_input': True, 'trace_as_metadata': True, 'type': 'str', 'value':\
        \ ''}, 'temperature': {'_input_type': 'SliderInput', 'advanced': True, 'display_name':\
        \ 'Temperature', 'dynamic': False, 'info': 'Controls randomness in responses',\
        \ 'max_label': '', 'max_label_icon': '', 'min_label': '', 'min_label_icon':\
        \ '', 'name': 'temperature', 'placeholder': '', 'range_spec': {'max': 1, 'min':\
        \ 0, 'step': 0.01, 'step_type': 'float'}, 'required': False, 'show': True,\
        \ 'slider_buttons': False, 'slider_buttons_options': [], 'slider_input': False,\
        \ 'title_case': False, 'tool_mode': False, 'type': 'slider', 'value': 0.1}}\n\
        \n# Extract runtime inputs (from other workflow steps)\nruntime_inputs = {}\n\
        if isinstance(input, dict):\n    for key, value in input.items():\n      \
        \  if key != '_stepflow_meta':  # Skip internal metadata\n            runtime_inputs[key]\
        \ = value\n\n# Call the UDF executor with the component configuration\nresult\
        \ = await context.call_component(\n    \"langflow://udf_executor\",\n    {\n\
        \        \"code\": \"\"\"\nfrom typing import Any\n\nfrom langchain_anthropic\
        \ import ChatAnthropic\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\
        from langchain_openai import ChatOpenAI\n\nfrom langflow.base.models.anthropic_constants\
        \ import ANTHROPIC_MODELS\nfrom langflow.base.models.google_generative_ai_constants\
        \ import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import\
        \ LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\n\
        from langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec\
        \ import RangeSpec\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io\
        \ import DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\n\
        from langflow.schema.dotdict import dotdict\n\n\nclass LanguageModelComponent(LCModelComponent):\n\
        \    display_name = \"Language Model\"\n    description = \"Runs a language\
        \ model given a specified provider.\"\n    documentation: str = \"https://docs.langflow.org/components-models\"\
        \n    icon = \"brain-circuit\"\n    category = \"models\"\n    priority =\
        \ 0  # Set priority to 0 to make it appear first\n\n    inputs = [\n     \
        \   DropdownInput(\n            name=\"provider\",\n            display_name=\"\
        Model Provider\",\n            options=[\"OpenAI\", \"Anthropic\", \"Google\"\
        ],\n            value=\"OpenAI\",\n            info=\"Select the model provider\"\
        ,\n            real_time_refresh=True,\n            options_metadata=[{\"\
        icon\": \"OpenAI\"}, {\"icon\": \"Anthropic\"}, {\"icon\": \"GoogleGenerativeAI\"\
        }],\n        ),\n        DropdownInput(\n            name=\"model_name\",\n\
        \            display_name=\"Model Name\",\n            options=OPENAI_MODEL_NAMES,\n\
        \            value=OPENAI_MODEL_NAMES[0],\n            info=\"Select the model\
        \ to use\",\n        ),\n        SecretStrInput(\n            name=\"api_key\"\
        ,\n            display_name=\"OpenAI API Key\",\n            info=\"Model\
        \ Provider API key\",\n            required=False,\n            show=True,\n\
        \            real_time_refresh=True,\n        ),\n        MessageInput(\n\
        \            name=\"input_value\",\n            display_name=\"Input\",\n\
        \            info=\"The input text to send to the model\",\n        ),\n \
        \       MultilineInput(\n            name=\"system_message\",\n          \
        \  display_name=\"System Message\",\n            info=\"A system message that\
        \ helps set the behavior of the assistant\",\n            advanced=True,\n\
        \        ),\n        BoolInput(\n            name=\"stream\",\n          \
        \  display_name=\"Stream\",\n            info=\"Whether to stream the response\"\
        ,\n            value=False,\n            advanced=True,\n        ),\n    \
        \    SliderInput(\n            name=\"temperature\",\n            display_name=\"\
        Temperature\",\n            value=0.1,\n            info=\"Controls randomness\
        \ in responses\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n\
        \            advanced=True,\n        ),\n    ]\n\n    def build_model(self)\
        \ -> LanguageModel:\n        provider = self.provider\n        model_name\
        \ = self.model_name\n        temperature = self.temperature\n        stream\
        \ = self.stream\n\n        if provider == \"OpenAI\":\n            if not\
        \ self.api_key:\n                msg = \"OpenAI API key is required when using\
        \ OpenAI provider\"\n                raise ValueError(msg)\n            return\
        \ ChatOpenAI(\n                model_name=model_name,\n                temperature=temperature,\n\
        \                streaming=stream,\n                openai_api_key=self.api_key,\n\
        \            )\n        if provider == \"Anthropic\":\n            if not\
        \ self.api_key:\n                msg = \"Anthropic API key is required when\
        \ using Anthropic provider\"\n                raise ValueError(msg)\n    \
        \        return ChatAnthropic(\n                model=model_name,\n      \
        \          temperature=temperature,\n                streaming=stream,\n \
        \               anthropic_api_key=self.api_key,\n            )\n        if\
        \ provider == \"Google\":\n            if not self.api_key:\n            \
        \    msg = \"Google API key is required when using Google provider\"\n   \
        \             raise ValueError(msg)\n            return ChatGoogleGenerativeAI(\n\
        \                model=model_name,\n                temperature=temperature,\n\
        \                streaming=stream,\n                google_api_key=self.api_key,\n\
        \            )\n        msg = f\"Unknown provider: {provider}\"\n        raise\
        \ ValueError(msg)\n\n    def update_build_config(self, build_config: dotdict,\
        \ field_value: Any, field_name: str | None = None) -> dotdict:\n        if\
        \ field_name == \"provider\":\n            if field_value == \"OpenAI\":\n\
        \                build_config[\"model_name\"][\"options\"] = OPENAI_MODEL_NAMES\n\
        \                build_config[\"model_name\"][\"value\"] = OPENAI_MODEL_NAMES[0]\n\
        \                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API\
        \ Key\"\n            elif field_value == \"Anthropic\":\n                build_config[\"\
        model_name\"][\"options\"] = ANTHROPIC_MODELS\n                build_config[\"\
        model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\n                build_config[\"\
        api_key\"][\"display_name\"] = \"Anthropic API Key\"\n            elif field_value\
        \ == \"Google\":\n                build_config[\"model_name\"][\"options\"\
        ] = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"\
        ][\"value\"] = GOOGLE_GENERATIVE_AI_MODELS[0]\n                build_config[\"\
        api_key\"][\"display_name\"] = \"Google API Key\"\n        return build_config\n\
        \n\"\"\",\n        \"template\": template_config,\n        \"component_type\"\
        : \"LanguageModelComponent\",\n        \"runtime_inputs\": runtime_inputs\n\
        \    }\n)\n\n# Return the result\nresult"
      function_name: null
      component_type: LanguageModelComponent
      display_name: Language Model
      description: 'Runs a language model given a specified provider. '
  depends_on:
  - create_ChatInput-X99lk_udf
  - create_Prompt-cxvt9_udf
- id: execute_ChatInput-X99lk
  component: langflow://langflow_udf
  input:
    blob_id:
      $from:
        step: create_ChatInput-X99lk_udf
      path: blob_id
    input:
      input_value:
        $from:
          input: message
  depends_on:
  - create_ChatInput-X99lk_udf
- id: execute_Prompt-cxvt9
  component: langflow://langflow_udf
  input:
    blob_id:
      $from:
        step: create_Prompt-cxvt9_udf
      path: blob_id
  depends_on:
  - create_Prompt-cxvt9_udf
- id: execute_ChatOutput-bKrF6
  component: langflow://langflow_udf
  input:
    blob_id:
      $from:
        step: create_ChatOutput-bKrF6_udf
      path: blob_id
    input:
      input_value:
        $from:
          step: execute_LanguageModelComponent-TsLa4
        path: result
  depends_on:
  - create_ChatOutput-bKrF6_udf
  - execute_LanguageModelComponent-TsLa4
- id: execute_LanguageModelComponent-TsLa4
  component: langflow://langflow_udf
  input:
    blob_id:
      $from:
        step: create_LanguageModelComponent-TsLa4_udf
      path: blob_id
    input:
      input_value:
        $from:
          step: execute_Prompt-cxvt9
        path: result
  depends_on:
  - create_LanguageModelComponent-TsLa4_udf
  - execute_ChatInput-X99lk
  - execute_Prompt-cxvt9
output:
  response:
    $from:
      step: execute_ChatOutput-bKrF6
    path: result
