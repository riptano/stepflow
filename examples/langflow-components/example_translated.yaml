description: Perform basic prompting with an OpenAI model.
input_schema:
  properties:
    message:
      default: Hello
      description: User input message
      type: string
  required:
  - message
  type: object
name: ExampleFlow
output:
  response: No output source found
steps:
- component: langflow://prompt_formatter
  id: prompt_cxvt9
  input:
    code: "from langflow.base.prompts.api_utils import process_prompt_template\nfrom\
      \ langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs\
      \ import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output,\
      \ PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils\
      \ import update_template_values\n\n\nclass PromptComponent(Component):\n   \
      \ display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template\
      \ with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\
      \n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\"\
      , display_name=\"Template\"),\n        MessageTextInput(\n            name=\"\
      tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n      \
      \      tool_mode=True,\n            advanced=True,\n            info=\"A placeholder\
      \ input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"\
      Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def\
      \ build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n\
      \        self.status = prompt.text\n        return prompt\n\n    def _update_template(self,\
      \ frontend_node: dict):\n        prompt_template = frontend_node[\"template\"\
      ][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"\
      ]\n        frontend_node_template = frontend_node[\"template\"]\n        _ =\
      \ process_prompt_template(\n            template=prompt_template,\n        \
      \    name=\"template\",\n            custom_fields=custom_fields,\n        \
      \    frontend_node_template=frontend_node_template,\n        )\n        return\
      \ frontend_node\n\n    async def update_frontend_node(self, new_frontend_node:\
      \ dict, current_frontend_node: dict):\n        \"\"\"This function is called\
      \ after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node,\
      \ current_frontend_node)\n        template = frontend_node[\"template\"][\"\
      template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n\
      \        _ = process_prompt_template(\n            template=template,\n    \
      \        name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"\
      ],\n            frontend_node_template=frontend_node[\"template\"],\n      \
      \  )\n        # Now that template is updated, we need to grab any values that\
      \ were set in the current_frontend_node\n        # and update the frontend_node\
      \ with those values\n        update_template_values(new_template=frontend_node,\
      \ previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\
      \n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
    template: Answer the user as if you were a GenAI expert, enthusiastic about helping
      them get started building something fresh.
    tool_placeholder: ''
- component: langflow://note
  id: note_0hpBn
  input: {}
- component: langflow://openai_chat
  id: languagemodelcomponent_TsLa4
  input:
    api_key: OPENAI_API_KEY
    code: "from typing import Any\n\nfrom langchain_anthropic import ChatAnthropic\n\
      from langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_openai\
      \ import ChatOpenAI\n\nfrom langflow.base.models.anthropic_constants import\
      \ ANTHROPIC_MODELS\nfrom langflow.base.models.google_generative_ai_constants\
      \ import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import\
      \ LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\n\
      from langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec\
      \ import RangeSpec\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io\
      \ import DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\n\
      from langflow.schema.dotdict import dotdict\n\n\nclass LanguageModelComponent(LCModelComponent):\n\
      \    display_name = \"Language Model\"\n    description = \"Runs a language\
      \ model given a specified provider.\"\n    documentation: str = \"https://docs.langflow.org/components-models\"\
      \n    icon = \"brain-circuit\"\n    category = \"models\"\n    priority = 0\
      \  # Set priority to 0 to make it appear first\n\n    inputs = [\n        DropdownInput(\n\
      \            name=\"provider\",\n            display_name=\"Model Provider\"\
      ,\n            options=[\"OpenAI\", \"Anthropic\", \"Google\"],\n          \
      \  value=\"OpenAI\",\n            info=\"Select the model provider\",\n    \
      \        real_time_refresh=True,\n            options_metadata=[{\"icon\": \"\
      OpenAI\"}, {\"icon\": \"Anthropic\"}, {\"icon\": \"GoogleGenerativeAI\"}],\n\
      \        ),\n        DropdownInput(\n            name=\"model_name\",\n    \
      \        display_name=\"Model Name\",\n            options=OPENAI_MODEL_NAMES,\n\
      \            value=OPENAI_MODEL_NAMES[0],\n            info=\"Select the model\
      \ to use\",\n        ),\n        SecretStrInput(\n            name=\"api_key\"\
      ,\n            display_name=\"OpenAI API Key\",\n            info=\"Model Provider\
      \ API key\",\n            required=False,\n            show=True,\n        \
      \    real_time_refresh=True,\n        ),\n        MessageInput(\n          \
      \  name=\"input_value\",\n            display_name=\"Input\",\n            info=\"\
      The input text to send to the model\",\n        ),\n        MultilineInput(\n\
      \            name=\"system_message\",\n            display_name=\"System Message\"\
      ,\n            info=\"A system message that helps set the behavior of the assistant\"\
      ,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"\
      stream\",\n            display_name=\"Stream\",\n            info=\"Whether\
      \ to stream the response\",\n            value=False,\n            advanced=True,\n\
      \        ),\n        SliderInput(\n            name=\"temperature\",\n     \
      \       display_name=\"Temperature\",\n            value=0.1,\n            info=\"\
      Controls randomness in responses\",\n            range_spec=RangeSpec(min=0,\
      \ max=1, step=0.01),\n            advanced=True,\n        ),\n    ]\n\n    def\
      \ build_model(self) -> LanguageModel:\n        provider = self.provider\n  \
      \      model_name = self.model_name\n        temperature = self.temperature\n\
      \        stream = self.stream\n\n        if provider == \"OpenAI\":\n      \
      \      if not self.api_key:\n                msg = \"OpenAI API key is required\
      \ when using OpenAI provider\"\n                raise ValueError(msg)\n    \
      \        return ChatOpenAI(\n                model_name=model_name,\n      \
      \          temperature=temperature,\n                streaming=stream,\n   \
      \             openai_api_key=self.api_key,\n            )\n        if provider\
      \ == \"Anthropic\":\n            if not self.api_key:\n                msg =\
      \ \"Anthropic API key is required when using Anthropic provider\"\n        \
      \        raise ValueError(msg)\n            return ChatAnthropic(\n        \
      \        model=model_name,\n                temperature=temperature,\n     \
      \           streaming=stream,\n                anthropic_api_key=self.api_key,\n\
      \            )\n        if provider == \"Google\":\n            if not self.api_key:\n\
      \                msg = \"Google API key is required when using Google provider\"\
      \n                raise ValueError(msg)\n            return ChatGoogleGenerativeAI(\n\
      \                model=model_name,\n                temperature=temperature,\n\
      \                streaming=stream,\n                google_api_key=self.api_key,\n\
      \            )\n        msg = f\"Unknown provider: {provider}\"\n        raise\
      \ ValueError(msg)\n\n    def update_build_config(self, build_config: dotdict,\
      \ field_value: Any, field_name: str | None = None) -> dotdict:\n        if field_name\
      \ == \"provider\":\n            if field_value == \"OpenAI\":\n            \
      \    build_config[\"model_name\"][\"options\"] = OPENAI_MODEL_NAMES\n      \
      \          build_config[\"model_name\"][\"value\"] = OPENAI_MODEL_NAMES[0]\n\
      \                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API\
      \ Key\"\n            elif field_value == \"Anthropic\":\n                build_config[\"\
      model_name\"][\"options\"] = ANTHROPIC_MODELS\n                build_config[\"\
      model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\n                build_config[\"\
      api_key\"][\"display_name\"] = \"Anthropic API Key\"\n            elif field_value\
      \ == \"Google\":\n                build_config[\"model_name\"][\"options\"]\
      \ = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"\
      ][\"value\"] = GOOGLE_GENERATIVE_AI_MODELS[0]\n                build_config[\"\
      api_key\"][\"display_name\"] = \"Google API Key\"\n        return build_config\n"
    input_value: ''
    model_name: gpt-4o-mini
    provider: OpenAI
    stream: false
    system_message:
      $from:
        step: prompt_cxvt9
      path: result
    temperature: 0.1
